import torch
import sounddevice as sd
import numpy as np
from transformers import AutoProcessor, AutoModelForCTC

# ÄÆ°á»ng dáº«n tá»›i checkpoint
model_path = "./results/checkpoint-315"

# Load model & processor
processor = AutoProcessor.from_pretrained(model_path)
model = AutoModelForCTC.from_pretrained(model_path)

# Cáº¥u hÃ¬nh ghi Ã¢m
sr = 16000   # sample rate (pháº£i khá»›p vá»›i model)
duration = 5  # sá»‘ giÃ¢y ghi Ã¢m

print("ğŸ™ï¸ Äang ghi Ã¢m... NÃ³i gÃ¬ Ä‘Ã³ nhÃ©!")
audio = sd.rec(int(duration * sr), samplerate=sr, channels=1, dtype='float32')
sd.wait()
print("âœ… Ghi Ã¢m xong!")

# Chuyá»ƒn sang dáº¡ng 1D numpy array
speech = np.squeeze(audio)

# Chuáº©n bá»‹ input
inputs = processor(speech, sampling_rate=sr, return_tensors="pt", padding=True)

# Dá»± Ä‘oÃ¡n
with torch.no_grad():
    logits = model(**inputs).logits

# Giáº£i mÃ£
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.batch_decode(predicted_ids)[0]

print("ğŸ—£ï¸ Káº¿t quáº£ nháº­n dáº¡ng:", transcription)
